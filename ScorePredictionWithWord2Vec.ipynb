{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-15.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages (from pyarrow) (1.26.4)\n",
      "Downloading pyarrow-15.0.1-cp312-cp312-macosx_11_0_arm64.whl (24.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-15.0.1\n",
      "Requirement already satisfied: numpy in /Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp312-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/beesamprajveenkumar/miniconda3/lib/python3.12/site-packages (from torch) (4.10.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.1-cp312-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Downloading MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.2.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install numpy\n",
    "!pip install sentencepiece\n",
    "!pip install torch\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hacknews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load Parquet data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhacknews.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Now, you can use the `data' DataFrame to analyse and manipulate the data.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hacknews.csv'"
     ]
    }
   ],
   "source": [
    "# Load Parquet data\n",
    "data = pd.read_csv('hacknews.csv')\n",
    "# Now, you can use the `data' DataFrame to analyse and manipulate the data.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns with the correct rows\n",
    "story_data = data[data[\"type\"] == \"story\"]\n",
    "\n",
    "# Extract the relevant columns\n",
    "relevant_data = story_data[[\"title\", \"score\"]]\n",
    "\n",
    "# Remove any null entries\n",
    "processed_data = relevant_data.dropna()\n",
    "\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array\n",
    "data_array = processed_data.to_numpy()\n",
    "print(data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data with sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the titles together one by one\n",
    "corpus = \"\\n\".join(data_array[:, 0])\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles\n",
    "title_array = data_array[:, 0]\n",
    "\n",
    "# Convert corpus to a file\n",
    "corpus = \"\\n\".join(title_array)\n",
    "with open(\"corpus.txt\", \"w\") as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input=\"corpus.txt\", model_prefix='hackernews', vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('hackernews.model')\n",
    "\n",
    "# Tokenize titles\n",
    "tokenized_titles = [sp.encode_as_pieces(title) for title in title_array]\n",
    "\n",
    "# Turn the tokens into tokenids\n",
    "id_titles = [[sp.piece_to_id(token) for token in title_tokens] for title_tokens in tokenized_titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply word2vec to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = sp.get_piece_size()\n",
    "embedding_dim = 128\n",
    "window_size = 2\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all the pairs of tokens and context windows for CBOW\n",
    "def generate_cbow_pairs(tokenized_sentences, window_size=2):\n",
    "    cbow_pairs = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        for index, word in enumerate(sentence):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context = [sentence[i] for i in range(start, end) if 0 <= i < sentence_length and i != index]\n",
    "            target = word\n",
    "            cbow_pairs.append((context, target))\n",
    "    return cbow_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `Dataset` class for generating pairs of tokens and context windows for CBOW one at a time, to use in the `DataLoader` to generate mini-batches\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_pairs, context_size):\n",
    "        self.cbow_pairs = cbow_pairs\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cbow_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.cbow_pairs[idx]\n",
    "        # Pad or trim the context to ensure uniform size\n",
    "        if len(context) < 2 * self.context_size:\n",
    "            context += [0] * (2 * self.context_size - len(context))  # Assuming 0 is the padding index\n",
    "        else:\n",
    "            context = context[:2 * self.context_size]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CBOW model, which is just an embedding layer, an average pooling, and a final output linear layer followed by a softmax activation\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        embedded = self.embeddings(context_words)\n",
    "        projection = torch.mean(embedded, dim=1)\n",
    "        out = self.linear(projection)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def embed(self, word):\n",
    "        self.eval()\n",
    "        embedding = self.embeddings(word)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the data generating functions we wrote earlier to our hacker news data\n",
    "cbow_pairs = generate_cbow_pairs(id_titles, window_size)\n",
    "cbow_dataset = CBOWDataset(cbow_pairs, window_size)\n",
    "cbow_dataloader = DataLoader(cbow_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, and optimizer\n",
    "cbow_model = CBOWModel(vocab_size, embedding_dim)  # Assuming CBOWModel is defined as before\n",
    "cbow_loss_function = nn.NLLLoss()\n",
    "cbow_optimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    with tqdm(cbow_dataloader, unit=\"batch\") as tepoch:\n",
    "        for context, target in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "            \n",
    "            cbow_model.zero_grad()\n",
    "            log_probs = cbow_model(context)\n",
    "            loss = cbow_loss_function(log_probs, target)\n",
    "            loss.backward()\n",
    "            cbow_optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished with total loss: {total_loss:.4f}\")\n",
    "    torch.save(cbow_model.state_dict(), os.path.join(\"cbow_weights\", f\"cbow_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all the pairs of tokens and context windows for Skip-gram\n",
    "def generate_skip_gram_pairs(tokenized_sentences, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                center_word_idx = sentence[center_word_pos]\n",
    "                context_word_idx = sentence[context_word_pos]\n",
    "                pairs.append((center_word_idx, context_word_idx))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `Dataset` class for generating pairs of tokens and context windows for Skip-gram one at a time, to use in the `DataLoader` to generate mini-batches\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return torch.tensor(center, dtype=torch.long), torch.tensor(context, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the data generating functions we wrote earlier to our hacker news data\n",
    "sg_pairs = generate_skip_gram_pairs(id_titles, window_size)\n",
    "sg_dataset = SkipGramDataset(sg_pairs)\n",
    "sg_dataloader = DataLoader(sg_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Skip-gram model, which is just an embedding layer, and a final output linear layer followed by a softmax activation\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_word):\n",
    "        embedded = self.embeddings(target_word)\n",
    "        out = self.linear(embedded)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def embed(self, word):\n",
    "        self.eval()\n",
    "        embedding = self.embeddings(word)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, and optimizer\n",
    "sg_model = SkipGramModel(vocab_size, embedding_dim)\n",
    "sg_loss_function = nn.NLLLoss()\n",
    "sg_optimizer = optim.Adam(sg_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center_word, context_word in tqdm(sg_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        sg_optimizer.zero_grad()\n",
    "        log_probs = sg_model(center_word)\n",
    "        loss = sg_loss_function(log_probs, context_word)\n",
    "        loss.backward()\n",
    "        sg_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Total Loss = {total_loss}\")\n",
    "\n",
    "    # Save the model state after each epoch\n",
    "    torch.save(sg_model.state_dict(), os.path.join(\"skipgram_weights\", f\"skipgram_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `Dataset` class for generating pairs of id-tokenised titles and scores one at a time, to use in the `DataLoader` to generate mini-batches\n",
    "class TitlesAndScores(Dataset):\n",
    "    def __init__(self, titles, scores):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids (list of list of words): Nested list where each sublist contains token IDs for a sentence.\n",
    "            scores (list of float): List of scores associated with each list of token IDs.\n",
    "        \"\"\"\n",
    "        self.titles = titles\n",
    "        self.tokenized_titles = [sp.encode_as_pieces(title) for title in self.titles]\n",
    "        self.id_titles = [[sp.piece_to_id(token) for token in title_tokens] for title_tokens in self.tokenized_titles]\n",
    "        self.scores = scores\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.scores)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.id_titles[idx], self.scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A padding function to make sure that every list of title token ids in a mini-batch is the same length, so that the batch can be tensorised\n",
    "def pad_collate(batch):\n",
    "    (id_titles, scores) = zip(*batch)\n",
    "    \n",
    "    # Padding the sequences with 0\n",
    "    padded_titles = pad_sequence([torch.tensor(title_ids) for title_ids in id_titles], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert scores to a tensor\n",
    "    tensor_scores = torch.tensor(scores, dtype=torch.float)\n",
    "\n",
    "    return padded_titles, tensor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset on our hackernews data\n",
    "sp_dataset = TitlesAndScores(data_array[:,0], data_array[:,1])\n",
    "\n",
    "# Create the dataLoader on our hackernews data\n",
    "sp_dataloader = DataLoader(sp_dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model for predicting scores given titles. \n",
    "# This particular architecture is intended to use one of the word2vec models we trained earlier (CBOW or Skip-gram) -- with its weights frozen -- to embed the input title tokens.\n",
    "# It then uses a classic neural network architecture to predict the scores given the embeddings.\n",
    "# This architecture has two hidden layers with ReLU activations, and a final output layer with no activation (a linear output layer) because we are doing a regression task. \n",
    "class ScorePredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, model, weights):\n",
    "        super(ScorePredictor, self).__init__()\n",
    "        self.embedding_model = model(vocab_size, embedding_dim)\n",
    "        self.embedding_model.load_state_dict(torch.load(weights))\n",
    "        self.embedding_model.eval()\n",
    "\n",
    "        self.hidden_1 = nn.Linear(embedding_dim, hidden_dim_1)\n",
    "        self.hidden_2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_dim_2, 1)\n",
    "\n",
    "    def forward(self, titles):\n",
    "        with torch.no_grad():  #To ensure no gradients are computed for the embedding model\n",
    "            embeddings = self.embedding_model.embed(titles)\n",
    "        pooled_embeddings = embeddings.mean(dim=1)  # Now x is of shape (batch_size, embedding_dim)\n",
    "        hidden_1_embeddings = self.hidden_1(pooled_embeddings)\n",
    "        activated_1_embeddings = self.relu(hidden_1_embeddings)\n",
    "        hidden_2_embeddings = self.hidden_2(activated_1_embeddings)\n",
    "        activated_2_embeddings = self.relu(hidden_2_embeddings)\n",
    "        score_predictions = self.output(activated_2_embeddings)\n",
    "        return score_predictions\n",
    "    \n",
    "    def predict(self, title):\n",
    "        self.eval()\n",
    "        self.forward(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_dim_1= 64\n",
    "hidden_dim_2 = 32\n",
    "sp_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the CBOW version of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, and optimizer\n",
    "sp_cbow_model = ScorePredictor(vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, model = CBOWModel, weights = \"cbow_weights/cbow_epoch_5.pth\")\n",
    "sp_cbow_loss_function = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "sp_cbow_optimizer = optim.Adam(sp_cbow_model.parameters(), lr=learning_rate)  # Using Adam optimizer\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(sp_epochs):\n",
    "    total_loss = 0\n",
    "    # Wrap your dataloader with tqdm for a progress bar\n",
    "    for inputs, targets in tqdm(sp_dataloader, desc=f'Epoch {epoch+1}/{sp_epochs}'):\n",
    "        \n",
    "        inputs = inputs.long()  \n",
    "        sp_cbow_model.zero_grad()\n",
    "        outputs = sp_cbow_model(inputs)\n",
    "        loss = sp_cbow_loss_function(outputs, targets.view(outputs.size()))  # Compute loss, ensuring target shape matches output\n",
    "        loss.backward()  # Backpropagation\n",
    "        sp_cbow_optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(f'Epoch {epoch+1}/{sp_epochs}, Loss: {total_loss/len(sp_dataloader)}')\n",
    "    \n",
    "    # Save the model state after each epoch\n",
    "    torch.save(sp_cbow_model.state_dict(), os.path.join(\"sp_cbow_weights\", f\"sp_cbow_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Skipgram version of Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, and optimizer\n",
    "sp_skipgram_model = ScorePredictor(vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, model = SkipGramModel, weights = \"skipgram_weights/skipgram_epoch_5.pth\")\n",
    "sp_skipgram_loss_function = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "sp_skipgram_optimizer = optim.Adam(sp_skipgram_model.parameters(), lr=learning_rate)  # Using Adam optimizer\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(sp_epochs):\n",
    "    total_loss = 0\n",
    "    # Wrap your dataloader with tqdm for a progress bar\n",
    "    for inputs, targets in tqdm(sp_dataloader, desc=f'Epoch {epoch+1}/{sp_epochs}'):\n",
    "        \n",
    "        inputs = inputs.long()  \n",
    "        sp_skipgram_model.zero_grad()\n",
    "        outputs = sp_skipgram_model(inputs)\n",
    "        loss = sp_skipgram_loss_function(outputs, targets.view(outputs.size()))  # Compute loss, ensuring target shape matches output\n",
    "        loss.backward()  # Backpropagation\n",
    "        sp_skipgram_optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(f'Epoch {epoch+1}/{sp_epochs}, Loss: {total_loss/len(sp_dataloader)}')\n",
    "    \n",
    "    # Save the model state after each epoch\n",
    "    torch.save(sp_skipgram_model.state_dict(), os.path.join(\"sp_skipgram_weights\", f\"sp_skipgram_epoch_{epoch+1}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for using our model to perform inference\n",
    "def score_predictor(title, vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, embed_model, embed_weights, weights):\n",
    "    model = ScorePredictor(vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, embed_model, embed_weights)\n",
    "    model.load_state_dict(torch.load(weights))\n",
    "    \n",
    "    title_ids = (torch.tensor([torch.tensor(sp.piece_to_id(token)) for token in sp.encode_as_pieces(title)])).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():  #To ensure no gradients are computed for the embedding model\n",
    "        score = model.predict(title_ids)\n",
    "\n",
    "    return score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing inference on the fake example title \"Hello world\"\n",
    "score_prediction = score_predictor(\"Hello world\", vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, embed_model=SkipGramModel, embed_weights=\"skipgram_weights/skipgram_epoch_5.pth\", weights=\"sp_skipgram_weights/sp_skipgram_epoch_10.pth\")\n",
    "print(score_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLX4.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
